 
\section{CHAPTER TWO: Literature Review}
\noindent This section provides a general literature review of major data mining techniques used in existing crime investigation systems,discuses some studies and systems at the Uganda Police related to this project and explains the concept of Complex Event Processing which is the  preferred technology for this project.

\subsection{Current Technologies in Crime Investigation Systems}

\noindent Existing crime investigation systems tend to vary in terms of their overall capabilities and technical operation. In one study \cite{two}the existence of prominent criminal investigation software like HOLMES2, BRAINS and Analysts' Notebook which are used by criminal analysts in the United Kingdom and Holland was acknowledged.  This category can be classified as early generation systems that mainly focused on analyzing evidence separately without linking multiple sets of evidence in order to solve crimes. They were also not designed to communicate with existing case management systems to facilitate data exchanges.These systems make use of relational database queries using SQL which is slow in terms of processing.

\noindent Second generation systems around the world rely heavily on data mining techniques to query large datasets for meaningful patterns in order to help investigators solve crimes. These methods however fall short in terms of supporting decision making largely due to poor processing speeds and querying algorithms.These systems mainly produce graphical representations of links between criminals and other crime entities.

\noindent Link Analysis is a technique used in data mining. These tools have for long been used by law enforcement agencies to identify, analyze and visualize relationships between crime entities. In a study \cite{five} , it is revealed that through association paths linking suspects and victims in crime, link analysis discovers information about motives and hence provides investigative leads.Link analysis requires an extensive amount of data preparation and is highly labour intensive.The performance of this technique detoriates with increasing data amounts and is therefore suitable for smaller observation sets.

\noindent In order to correlate entities , investigators must manually search for associations by examining a large number of documents that may range from structured database records of crime incidents to unstructured report narratives. Link Analysis is similar to the breadth-first search alogrithm in which a search tree rooted at one of the known enitities. The process involves examining one or more documents and consumes a considerable amount of time.

\noindent Another problem with link analysis is high branching factors as a result of very many asociations between entities which increases the complexity of the search alogrithm. Also paths found during analysis may not be useful as they may contain unimportant links.Link Anlaysis heavily relies on  domain knowledge and experience making it very difficult to automate the process. Investigators need to determine whether an association between two crime entities is important for uncovering investigative leads. As a result this is a highly costly technique though effective in a way.

\noindent Another technique used in crime pattern detection involves several data mining steps like hotspot detection, crime clock, crime comparison and crime pattern visualization. Numerous algorithms are used to relate multiple crime scenes, represent a number of crime scenes on a daily basis, compare different crimes to estimate growth rates and visualize the changes in crime occurrence frequencies \cite{three}.

\noindent A study on crime network analysis \cite{fifteen} suggests that law enforcement agencies need to deploy reliable data and sophisticated tools as critical tools in the discovering useful patterns in data. The study introduces a data mining technique called Social Network Analysis (SNA) which is used to discover hidden patterns in large volumes of crime related data. An approach of SNA referred to as block modeling is used in criminal networks to reveal associations between subgroups based on a link density measure. Discovery of new structural patterns during this process can enable prevention of crimes and also modify conventional view of certain crimes by investigators.This approach is  expected to provide more advanced analytical functionality
to assist crime investigation.Sophisticated structural analysis tools are needed to go from merely drawing networks to mining large volumes of data to discover useful knowledge about the structure and organization of criminal networks \cite{fifteen}.


\subsection{Related Work in Uganda}

\noindent In Uganda, some studies  have been conducted in the area of crime investigation. One such study \cite{sixteen} discusses a model for forensic investigations that performs detection of incidents through system monitoring and performs data analysis to unearth the crime scene, suspect and how the crime was perpetrated. The study proposed a new model based on five iterative phases that were meant to strengthen the crime detection and analysis process. The model suggested depicts the forensic process as iterative as opposed to linear,differentiates the investigations at the primary (suspect) and secondary (victim) crime scenes, introduces a new phase (Traceback) that would reflect the process of arriving at the perpetrators scene, re-defines the phases in the physical and digital crime scene investigation phases in the previous models, re-defines the Deployment phase in the previous model to include the physical and digital
crime investigations,reserves only one reconstruction (at the end) but provides for investigative hypotheses during the entire process and is suitable for cyber-crime investigations.


\noindent Another study \cite{seventeen} on crime prevention suggested a combined application of data mining techniques alongside GIS (Geographical Information Systems) to discover crime data in disorganized settings like Uganda. Spatial Point Patterns (SPP) based on coordinates of events such as locations of crime incidences and the time of occurrence are used.All or a sample of  point pattern may be plotted on the map. The aim of  SPP analysis is to detect whether the point pattern is distributed at random, clustered or regular. SPP is typically interpreted as analysis of  clustering. A dot map is commonly used to represent SPP. The tool effectively used for analysis of  clustering effects is the K function. This method assesses clustering of  crime incidences in detection of  hot spots wheretime and space relationship analysis is required, the methods used are Knox’s method,Mantel’s Method and K-nearest neighbour method. 

\subsection{Existing Systems at Uganda Police}

\noindent The Uganda Police is currently conducting a pilot study at selected Police Stations to ascertain the effectiveness of a newly developed web-based system called "A CRIME RECORD MANAGEMENT SYSTEM". This system is hosted at the Information Technology Department of Uganda Police at the HeadQuarters in Naguru,Kampala and enables capturing of case details , statements recorded by complainants and also scanned(image) copies of supporting documents attached to a particular case. The major output of the system are the case statements which are generated and forwarded to investigating officers by the station commander to follow through on the cases.In the current state the system cannot integrate with the tool proposed in this study but can be improved to capture supporting evidence records in a structured format from the current image format.

\noindent The other prominent system used by the Uganda Police is called "THE SUSPECT PROFILING SYSTEM" which is used to among other things search and match criminal data as well as track suspicious changes in their profiles form time to time. The tracing is sometimes done using biometric features like fingerprints especially when tracking a suspect's criminal history. This system also performs online monitoring of cyber criminals based on data stored previously in the database.  


\subsection{The Crime Investigation Process}

\noindent An investigation is an examination, a study, a survey and a research of facts and/or circumstances,situations, incidents and scenarios, either related or not, for the purpose of rendering a conclusion of proof. An investigation, therefore, is based upon a complete and whole evaluation and not conjecture, speculation or supposition.Crime detection and investigation is both an art and a science; a collaboration of common sense, judgment, intellect, experience and an innate intuitiveness along with a grasp of relative technical knowledge. The criminal investigator must continually apply those skills, acquired through study and experience, to the examination and observation of the criminal and his behavior, as well as his social and physical environment \cite{twentyseven}.

\noindent There are several basic types of investigations that law enforcement personnel may undertake in the routine discharge of their duties:  Investigations of incidents, which are violations of laws and/or ordinances that include; criminal acts (robbery, assaults, larceny, burglary, murder, illegal weapons, etc…) and traffic accident investigations (serious injuries, likely to die, property damage). Personnel investigations into the background, character and suitability of persons in an effort to determine their eligibility for positions of public trust.  Investigations of illegal conditions or circumstances, which if left unchecked would cause an increase in traditional crimes. These conditions may include the following: narcotics sales, illegal weapons trafficking, vice type crimes (prostitution, gambling), street gang activity, organized crime, terrorist front activities, fraud and con games, identity theft and computer crimes. Although many of these conditions would dictate self-initiated investigations based upon intelligence rather than reacting to a citizen crime complaint, there are however, times that investigations will in fact result from such individual crime complaints \cite{twentyseven}. 

\noindent The official purpose of criminal investigation in most countries is to retrieve information that can be used as evidence in court.The obtained evidence then becomes the basis for judges' and juries' decisions concerning the guilt of prosecuted defendants and the sentences imposed on those found guilty. From the above description it is evident that investigative activities cannot be fully understood if viewed detached from its context, but should be seen as intertwined with other components of the criminal justice system. Therefore, it is helpful to consider how investigators' work relates to the prosecution process. In a prosecutor's application for a summons, a claim is to be made concerning the criminal behavior of a defendant in the past. A prerequisite for issuing a summons is, first and foremost, that the identity of the defendant is clear. Furthermore, the criminal act must be specified with regard to the time and place of the offense. Finally, the circumstances surrounding the offense should be detailed and proven to fulfill the legal requisites for the specified crime classification. The investigative work carried out by the police authority serves to provide the prosecutor with all the above information \cite{twentysix}.



\subsection{Complex Event Processing}

\noindent A Complex event is an event that abstracts or aggregates simple (or member) events \cite{seven}. Simple and complex events are normally represented in linear ordered sequences called Event Streams. These streams are usually bound by time intervals and may contain different types of events.

\noindent Complex Event Processing (CEP) is defined as the process of detecting complex events using continuously incoming events on a lower abstraction level \cite{four}. This study  justifies the need for CEP given the fact that single events on their own may not be sufficient in determining certain patterns. 

\noindent CEP is a foundational technology for detecting and managing the events that happen in event driven enterprises. It is a collection of methods, tools and techniques applied in processing events as they happen. In order to achieve a lot from CEP, happenings of events in enterprises need to be well understood. This can be achieved by organizing events into structures or hierarchies, identifying relationships among events (causal, time, aggregation) and organizing events in different views from different personnel. In CEP, higher- level knowledge is derived from lower-level events which are a combination of various occurrences. CEP can be viewed in two types, the first one involving specification of complex events as patterns and detecting them effectively, whereas the other type involves detecting new patterns as complex events. In the first case, event query languages offer convenient means to specify complex events and detect them efficiently. In the second case, machine learning and data mining methods are applied to event streams.

\noindent Detection of complex events is, of course, not an end in itself; an event-driven information system should react automatically and adequately to detected events. Typical reactions include notifications (e.g., to another system or a human user), simple actions (e.g., buy stocks, activate fire extinguishing installation), or interaction with business processes (e.g., initiation of a new process, cancellation or modification of a running process).


\begin{center}
\begin{figure}[h]
\centerbmp{14cm}{5cm}{eda2.png}
\caption{Event Driven Architecture \cite{twentytwo}}

\end{figure}
\end{center}



\noindent A study about the history of CEP \cite{eighteen} traces its roots to university and company research groups in the late 90's which were involved in the areas of active databases, event driven simulation, networking and event processing in middleware. This explains partly why the CEP query languages are based on SQL syntax having been influenced by research on active databases. CEP products at that point did not generate much interest until the late 2000's when CEP was deployed as add-ons on SOA architectures and ESBs.

\noindent Databases are distinct from event queries used in CEP because of the latter's ability to continuously detect events as they happen rather than just acting on stored datasets. Event processing languages need to enable the possibility of joining several individual events together, so that their combined occurrences over time yield a complex event and complex events must contain the element of time, to track times when events occur.One study \cite{twenty} introduced the need for revision of events in cases of erroneous data. In practice, there are a number of reasons requiring revisions in event stream processing. For example, an event was reported by mistake, but did not happen in reality (and the mistake was realized later); an event happened, but it was not reported (due to failure of either a sensor, or failure of the event transmission system); or an event was triggered and later revoked due to the transaction failure. Also very often streaming data sources contend with noise (e.g., financial data feeds, Web streaming data, updates etc.) resulting in erroneous inputs and, therefore, erroneous complex event results.

\noindent Through Complex Event Processing (CEP), companies and organizations can manage processes in close to real-time. It is however noted that due to the complexity of generic event processing frameworks offered by the industry, the configuration and setup of CEP applications are left to external experts who are more knowledgeable in complex event logic. A CEP application retrieves events for all noteworthy incidents in the business environment. In various parts of the application, event-pattern rules are applied on the incoming event stream to detect relevant patterns, e.g., an uptrend in application errors or execution delays. In response to such patterns, the CEP engine proactively intervenes in the business environment, e.g., by temporarily allocating additional resources, throttling uncritical business tasks or notifying system administrators \cite{eighteen}.

\noindent Pattern matching is a key feature of all CEP technologies which involves finding subsets of data matching a given pattern and also relationships between those subsets. A study about CEP under uncertainty \cite{nineteen} explains the role pattern matching plays in allowing users to look beyond individual events and find specific collection sets.

\noindent Solution templates are proposed \cite{ten}to perform data mining procedures on historical data to identify event patterns besides real-time monitoring. The central concept of any template's event processing infrastructure is the event processing map, a predefined orchestration of event adapters and event services. Event adapters may be considered the actual interface to the underlying source system: Depending on their implementation, event adapters translate real-world actions (such as a user actually placing a bet in an online gambling platform) into event representations of a certain event type, and vice versa. Event services receive events from event adapters or other event services, process them based on implementation of specific logic, and respond back to the map.Detecting events is based on some considerations like, some events sharing time elements, the order of events, time bounds within events and detection of events of long time lags. To gain an insight into processes there is need to include the following components in CEP application, facilities (graphical or textual) for precise description of complex patterns of events, scalable performance, modular rules engines to detect complex patterns of events, facilities for defining and composing event pattern triggered rules for pattern abstraction.


\newpage
\subsection{Event Processing Engines}

\noindent An analysis of Event Processing Languages \cite{four} revealed the need to shift from using general-purpose languages like C, Java, C++ e.t.c for CEP applications due to low-level complexities. Using such languages along with complexities like data structures and algorithms can only complicate the development process. The study provided a detailed analysis of existing CEP programming Languages and platforms based on their expressivity and integration capabilities. Expressivity is measured by one of the following abilities, filtering streams by event type, processing a subset of events (windows), data extraction and aggregation of data over events, performing conjunctions and disjunctions, show temporal relations between events, showing causality of events, negation and counting of events, event instance selection and consumption to prevent reuse of events in pattern detection and integration of event data and non-event data (data from outside). Languages are also grouped into the categories of data stream query languages, composition-operator-based languages, production rule languages and logical formulas.

\noindent STREAM (Stanford Stream data Manager) is a language whose focus was to develop methods to manage and query data in data streams and was a result of a research project at Stanford University. The project also produced a CEP engine called STREAM and an Event Query Language called Continuous Query Language to query events. STREAM was a basis for other data stream languages like Esper and its querying syntax resembles SQL very strongly.

\noindent Borealis is a CEP engine developed at Brandeis University and MIT that uses a "boxes and arrows" approach. Queries are described graphically with queries as boxes and streams as arrows connecting boxes. The approach was first used in an earlier engine, Aurora. The main difference between Borealis and stream languages is the focus on query evaluation that Borealis offers resulting in less abstract queries than STREAM.

\noindent Active Middleware Technology (AMiT) enables IBM middleware to become event-based. This technology is implemented in several products, most notably extending Web-Sphere Broker with CEP capabilities. As WebSphere is a commercial product, it is not freely available (requires registration .Basic events are declared with their attributes in event tags. Lifespans are windows defined by two events, an initiator and a terminator event. Lifespan types are therefore declared by referencing start and end event types. Whenever an event matching the initiator specification is detected, a new lifespan of this type is opened, and when an event matching the terminator specification is detected, the lifespan is closed. Complex events are called situations. A situation consists of at least one data attribute (it has to carry at least one kind of information), exactly one operator, and a lifespan type. Situations are only tried to be detected in lifespans of its type. A lifespan may be referenced by multiple situations

\noindent RuleCore is a CEP engine developed by Analog Software, building on research at the University of Skyde. As the name suggests, rules are the central concept of ruleCore. The ruleCore engine processes events using ECA (Event-Condition-Action) rules that consist of three parts: for every event (basic or complex), check a condition; if it is true, execute the action. ruleCore has two implementations; an open source variant called ruleCore, released under the terms of the GPL; as well as a commercial version called ruleCore CEP Server. RuleCore uses so-called detector trees for event detection. Leaf detector nodes (detector nodes without children) detect single events (they pick up events of their type). They are inactive until an event of their type is delivered to the rule (usually by entering the system, although exceptions are mentioned in the next paragraph), after which point they are always active. To detect complex events, a detector tree is built: the leaves detect simple events, and inner nodes detect complex events depending on whether its children detected events.

\noindent SASE+ is a CEP system developed at the University of Massachusetts, Amherst. It is an extension of the older SASE system. The system is designed for event streams with many events per time unit and also queries using large time windows, creating new issues regarding efficient query execution. The project's purpose is to devise techniques for high-performance querying of event streams, using a declarative, composition-operator-based language. Although SASE+ is an agile language and concentrates only on pattern matching on streaming data, the pattern matching properties of SASE+ can be used in more general contexts.

\noindent Esper is an open-source CEP engine, developed by EsperTech Inc. and volunteers, released under the GNU General Public License (GPL v2). As stated on the official web site\cite{twentyfive}, it is designed for CEP and Event Stream Processing (ESP).There are two implementations of Esper, Esper for Java and NEsper for .NET. Both supply an API to access the engine features, such as deploying queries, sending events into the engine and retrieving events out of the engine, in their respective language. Events are objects in their respective language; for Esper, events can be instances of java.util.Map,org.w3c.dom.Node (Java representations of XML documents), or other Java objects. Regardless of the implementation language, queries are stated in a SQL-like language called Event Processing Language (EPL).

\noindent Cayuga is a research CEP engine developed at Cornell University. It sets itself apart from other engines in that it deliberately sacrifices expressivity for performance, targeting applications running large numbers of queries. It is free software, available under the terms of the BSD license. Cayuga uses an Event Query Language called Cayuga Event Language (CEL). While its syntax resembles SQL, like many data stream languages, it also offers patterns, although using a different approach compared to Esper, inspired by regular expressions.

\noindent Drools, also known as JBoss rules, is a production-quality business rule management system, including a production rule engine. It is free software, released under the Apache License. The Drools engine is implemented in Java, as is JBoss, and is also controlled using that language. Initialization of the engine and deployment of rules is implemented in Java. Also, as unusual for production rule engines, rules never fire by themselves, but are issued to do so by the Java program that controls the engine. In addition, Drools can be extended by defining so-called Domain Specific Languages. These are languages that may have a different syntax than the standard Drools syntax to write queries in. Rules in Domain Specific Languages are then translated into the Drools language when inserted into the engine.

\noindent XChangeEQ is a research Event Query Language. It is developed at the University of Munich and designed for automated reasoning on the Semantic Web. XChangeEQ introduces a new style of event querying. It separates event query features into four so-called dimensions: Data extraction, event composition, temporal relationships, and event accumulation. Most operators belong to exactly one of these dimensions. This was done to define clear semantics. As XChangeEQ is designed for use on the Web, it works best at processing tree-structured events, such as XML messages. Queries are generally structured like the XML representations of the events queried. For querying simple events, it embeds the Xcerpt language. Xcerpt queries apply patterns to XML documents, similar to templates. Change is a reactive programming language. Using Event-Condition-Action rules, it allows Web sites to react to changes at other Web sites, for example by updating its own data.

\noindent TelegraphCQ, from the University of California at Berkeley, is designed to provide event processing capabilities alongside relational database management capabilities by utilizing the PostgreSQL open source code base. The existing architecture of PostgreSQL is modified to allow for continuous queries over streaming data. Several components of the PostgreSQL engine underwent very little modification, while others were significantly changed. The most significant component of the TelegraphCQ system is the "wrapper," which allows for data to be pushed or pulled into the Telegraph processing engine, and custom wrappers allow for data to be obtained from any data source .

\noindent BEA Systems in 2007 introduced of their WebLogic Real Time and WebLogic Event Server systems. More specifically, their Event Server technology is a focus on event-driven service oriented architecture which provides a response to events in real-time. As part of the package, they provide a complete event processing and event-driven service-oriented architecture infrastructure that supports high-volume, real-time, complex, event-driven applications. This is one of the few commercial offerings of a complete, integrated solution for event processing and service-oriented architectures.

\noindent Truviso is a commercial event processing engine that is based on the research toward the Telegraph CQ project at UC Berkeley. The"claim to fame" for Truviso is that it supports a fully functional SQL, and integrates PostgreSQL relational database alongside a stream processing engine. The integration of PostgreSQL leads to other aspects of the Truviso system. The queries are simply standard SQL with extensions that add functionality for time windows and event processing. Carried over from PostgreSQL are user-defined functions, as well as JDBC and ODBC interfaces. In addition, the use of an integrated relational database allows for easy caching, persistence, and archival of data streams, as well as queries that include not only real-time data, but also the historical data \cite{nine}.
 
